\section{Lecture 4. Eigenvalues and Eigenvectors}
\begin{multicols}{2}
\begin{definition}[Eigenvalues and Eigenvector]
    Let $A\in\mathbb{R}^{m\times m}$, if a scalar $\lambda\in\mathbb{C}$ and a nonzero vector $v\in\mathbb{C}^m$ satisfy the equation 
    \[
        Av=\lambda v
    \]
    then $\lambda$ is called the eigenvalue of $A$ and $v$ is called the eigenvector of $A$ associated with $\lambda$.
\end{definition}
$\Longrightarrow$ 
\[
    \underbrace{(A-\lambda I)}_{\text{\rm singular}}v = 0
\]
$P_A(\lambda)\triangleq det(A-\lambda I)=\prod_{i=1}^m(\lambda-\lambda_i)$ is a polynomial of $\lambda$ with degree $m$.
The equation $P_A(\lambda)\triangleq det(A-\lambda I)=0$ is called the characteristic euqation of $A$.

For each $\lambda_i$ find $v_i$ such that 
\[
    (A-\lambda_i I)v_i = 0
\]
$v_i\in Null(A-\lambda_i I)$ and $Null(A-\lambda_i I)$ is the eigenspace for $\lambda_i$
\begin{enumerate}
    \item [-] Even if $A\in R^{m\times m}$ is real-valued, $\lambda_i$ and $v_i$ could be complex.
    \emph{e.g.} 
    \[
        A=\begin{bmatrix}
            0& -1 \\ 1 & 0
        \end{bmatrix}\quad \Longrightarrow \quad \lambda = \pm j
    \]
    \item [-] But if $\lambda_i\in\mathbb{R}$ is real-valued, $v_i\in\mathbb{R}^m$ should be in $\mathbb{R}^m$.
\end{enumerate}

\subsection{Diagonalizability}
\subsubsection{Similar matrices}
\begin{definition}[Similar matrices]
    $A\in\mathbb{R}^{m\times m}$ and $B\in\mathbb{R}^{m\times m}$ are similar to each other if there exists a nonsingular $S\in\mathbb{R}^{m\times m}$ such that 
    \[
        B=SAS^{-1}
    \]
\end{definition}
If $A$ and $B$ are similar to each other, then they have the same eigenvalues. \\
\begin{proof}
    \[
        \begin{array}{ll}
            P_B(\lambda)&=det(B-\lambda I) \\
            &= det(SAS^{-1}-\lambda I) \\
            &= det(S(A-\lambda I)S^{-1}) \\
            &= det(S)det(A-\lambda I)det(S^{-1}) \\
            &= det(A-\lambda I) \\
            &= P_A(\lambda)
        \end{array}    
    \]
\end{proof}

\subsubsection{Diagonalizable matrix}
\begin{definition}[Diagonalizable]
    A matrix $A\in\mathbb{R}^{m\times m}$ is diagonalizable if $A$ is similar to a diagonal matrix. \\
\end{definition}
$\Longrightarrow$ (\textbf{Eigenvalue decomposition}) \\
There exists an invertible matrix $V\in\mathbb{R}^{m \times m}$ and a diagonal matrix $D\in \mathbb{R}^{m \times m}$ s.t.
\[
    \underbrace{A = VDV^{-1}}_{\text{\rm eigenvalue decomposition (EVD)}}
\]
$\Longrightarrow$
\[
    AV=VD
\]
\[V = [v_1,v_2,...,v_m]\quad , \quad D = \left[\begin{array}{ccc}
    \lambda_1 && \\
    & \ddots & \\
    && \lambda_m
\end{array}\right]\]
$\Longleftrightarrow$ \\
\[
    Av_i = \lambda_i v_i ,\quad \forall i=1,...,m
\]
$\Longrightarrow$ \\
$A$ is diagonalizable \emph{iff.} there is a set of linear indep. vectors which are engenvectors of $A$.
\textbf{Properties} \\
If a matrix $A\in\mathbb{R}^{m\times m}$ is diagonalizable
\begin{enumerate}
    \item $det(A)=\prod_{i=1}^m \lambda_i$
    \item $Tr(A)=\sum_{i=1}^m \lambda_i$
    \item $rank(A)=$ the number of non-zero eigenvalues
\end{enumerate}
\begin{proof} 
    \begin{enumerate}
        \item $det(A)=det(VDV^{-1})=det(V)det(D)det(V^{-1})=det(D)=\prod_{i=1}^m\lambda_i$
        \item $Tr(A)=Tr(VDV^{-1})=Tr(DV^{-1}V)=Tr(D)=\sum_{i=1}^m\lambda_i$
    \end{enumerate}
\end{proof}
\subsection{When Diagonalizable}
\subsubsection{Distinct eigenvalues}
Let $\{\lambda_1,\lambda_2,...,\lambda_k\}_{k\in M}$ ($k\geq 2$) be a subset of eigenvalues of $A$ and $\lambda_i\neq \lambda_j$ $\forall i\neq j$. 
Then the corresponding eigenvectors, denoted with $v_1,...,v_k$ respectively, are linearly independent. 

If $A$ has $\lambda_1\neq\lambda_2\neq\cdots\neq\lambda_k$, then the corresponding eigenvectors $v_1,...,v_k$ are linear independent. \\
\begin{proof}
    \par Suppose $v_1,...,v_k$ are linearly \textbf{dependent}. Then there exists $\alpha_1,\alpha_2,...,\alpha_k$ not all zero 
    (without losing generality, assume $\alpha_1\neq 0$) s.t. 
    \[
        \sum_{i=1}^k \alpha_i v_i = 0
    \]
    Then
    \[
        A\cdot \left(\sum_{i=1}^k \alpha_i v_i\right) = \sum_{i=1}^k \alpha_i A v_i = \sum_{i=1}^k \alpha_i \lambda_i v_i = 0
    \]
    \begin{equation}
        \sum_{i=1}^{k-1} \alpha_i \lambda_i v_i + \alpha_k \lambda_k v_k = 0 \tag{1}
    \end{equation}
    On the other hand,
    \[
        \lambda_k\cdot \left(\sum_{i=1}^k \alpha_i v_i\right) = \sum_{i=1}^{k} \alpha_i \lambda_k v_i = 0
    \]
    \begin{equation}
        \sum_{i=1}^{k-1} \alpha_i \lambda_k v_i + \alpha_k \lambda_k v_k = 0 \tag{2}
    \end{equation}
    $(1)-(2)$:
    \[
        \sum_{i=1}^{k-1} \alpha_i (\lambda_i - \lambda_k) v_i = 0
    \]
    Then
    \[
        \begin{array}{rl}
            A\cdot \left(\sum_{i=1}^{k-1} \alpha_i (\lambda_i - \lambda_k) v_i\right)&=\sum_{i=1}^{k-1} \alpha_i (\lambda_i - \lambda_k) Av_i \\
            &=\sum_{i=1}^{k-1} \alpha_i (\lambda_i - \lambda_k) \lambda_i v_i \\
            & = 0
        \end{array}
    \]
    \begin{equation}
        \sum_{i=1}^{k-2} \alpha_i (\lambda_i - \lambda_k) \lambda_i v_i + \alpha_{k-1}(\lambda_{k-1}0\lambda_k)\lambda_{k-1}v_{k-1}=0 \tag{3}
    \end{equation}
    On the other hand
    \[
        \lambda_{k-1}\cdot \left(\sum_{i=1}^{k-1} \alpha_i (\lambda_i - \lambda_k) v_i\right)=\sum_{i=1}^{k-1} \alpha_i (\lambda_i - \lambda_k)\lambda_{k-1}v_i =0 
    \]
    \begin{equation}
        \sum_{i=1}^{k-2} \alpha_i (\lambda_i - \lambda_k) \lambda_{k-1} v_i + \alpha_{k-1}(\lambda_{k-1}0\lambda_k)\lambda_{k-1}v_{k-1}=0 \tag{4}
    \end{equation}
    $(3)-(4)$:
    \[
        \sum_{i=1}^{k-2} \alpha_i (\lambda_i - \lambda_k)(\lambda_i - \lambda_{k-1}) v_i = 0
    \]
    Repeat until $i=1$ 
    \[
        \alpha_1\left(\prod_{i=1}^k(\lambda_1 - \lambda_i)\right)v_1 = 0
    \]
    Since $\alpha_1\neq 0$ and $\prod_{i=1}^k(\lambda_1 - \lambda_i)\neq 0$, so $v_1 = 0$ (Contradict!) \\
    Therefore, $v_1,...,v_k$ are linearly \textbf{independent}
\end{proof}

If $A\in\mathbb{R}^{m\times m}$ has $m$ distinct eigenvalues, then $A$ is diagonalizable.

\subsubsection{Repeated eigenvalues}
Let $\lambda$ be an eigenvalue of $A$ repeated $r$ times. Define
\begin{itemize}
    \item [-] Algebra multiplicity: $r$.
    \item [-] Geometric multiplicity: $s=dim(Null(A-\lambda I))$
\end{itemize}
Then
\[
    s\leq r
\]
\begin{proof} \\
    Let $dim(Null(A-\bar{\lambda I}))=s$ and $\{q_1,...,q_s\}$ be an orthogonal basis for $Null(A-\bar{\lambda} I)$. \\
    Then $Q_1 = [q_1,...,q_s]\in\mathbb{R}^{m\times s}$ is semi-unitary and $\exists Q_2\in\mathbb{R}^{m\times (m-s)}$ s.t. 
    $Q\triangleq [Q_1,Q_2]$ is unitary.\\
    Consider 
    \[
        Q^HAQ = \begin{bmatrix}
            Q_1^H AQ_1 & Q_1^HAQ_2 \\
            Q_2^H AQ_1 & Q_2^HAQ_2
        \end{bmatrix} = \begin{bmatrix}
            \bar{\lambda} I_s & Q_1^HAQ_2 \\
            0 & Q_2^HAQ_2
        \end{bmatrix}
    \]
    Then
    \[
        \begin{aligned}
            P_A(\lambda) & =P_{Q^HAQ}(\lambda) \\
            & = det(Q^HAQ-\lambda I) \\
            & = det\left(\begin{bmatrix}
                (\bar{\lambda}-\lambda)I_s & Q_1^HAQ_2 \\
                0 & Q_2^HAQ_2-\lambda I_{m-s}
            \end{bmatrix}\right) \\
            & = \underbrace{(\bar{\lambda}-\lambda)^s}_{\bar{\lambda}\text{ \rm repeats } s \text{ \rm times}} det(Q_2^HAQ_2-\lambda I_{m-s}) \\
            & = 0
        \end{aligned}
    \]
    So \[r\geq s\]
\end{proof}
If $s=r$, $A$ is diagonalizable.
\subsection{Symmetric/Hermitian Matrices}
\begin{itemize}
    \item [-] \textbf{Symmetric} $A=A^T$ $A\in\mathbb{R}^{m\times n}$\\
    \item [-] \textbf{Hermitian} $A=A^H$ $A\in\mathbb{C}^{m\times n}$
\end{itemize}
\textbf{Property 1}: The eigenvalues of a Hermitian matrix $A\in\mathbb{C}^{m\times n}$ are real-valued. \\
\begin{proof} \\
    Let $\lambda$ be the eigenvalue of $A$, $v$ be the eigenvector of $A$ and $\|v\|_2=1$. \\
    $\Longrightarrow$ $Av=\lambda v$ \\
    $\Longrightarrow$ $v^HAv=\lambda v^Hv=\lambda$ \\
    $\overset{\text{\rm take} H}{\Longrightarrow}$ $v^HA^Hv=\lambda^*$ \\
    Since $A$ is Hermitian, $A=A^H$ and hence $\lambda = \lambda^*$ \\
    So $\lambda$ is real-valued
\end{proof}
\textbf{Property 2}: If $A\in\mathbb{C}^{m\times n}$ is Hermitian and that 
\begin{itemize}
    \item [-] $\lambda_i\neq\lambda_j$ be two eigenvalues,
    \item [-] $v_i, v_j$ be the corresponding eigenvectors.
\end{itemize}
Then the eigenvectors are mutually orthogonal. \\
\begin{proof}
    \[
    \left\{\begin{array}{l}
        v_i^HAv_j = v_i^H(\lambda_jv_j)=\lambda_jv_i^Hv_j \\
        v_j^HAv_i = v_j^H(\lambda_iv_i)=\lambda_iv_j^Hv_i \overset{\text{\rm take} H}{\Longrightarrow} v_i^HA^Hv_j = \lambda_iv_i^Hv_j
    \end{array}\right.
    \]
    Since $v_i^HA^Hv_j=v_i^HAv_j$, so we have
    \[
        (\lambda_i-\lambda_j)v_i^Hv_j = 0 \Longrightarrow v_i^Hv_j = 0
    \]
\end{proof}

\subsubsection{Unitarily diagonalizable}
If Hermitian/symmetric $A$ is diagonalizable
\[
    A=VDV^{-1}
\]
where
\begin{itemize}
    \item [-] $V$ contains orthogonal eigenvectors and is unitary matrix,
    \item [-] $D$ real-valued diagonal eigenvalues.
\end{itemize}
Therefore, $VV^H=I$ and $V^{-1}=V^H$. Thus, 
\[
    A = VDV^H
\]
And we say $A$ is unitarily diagonalizable.

\subsubsection{Schur triangularization}
Any $A\in\mathbb{C}^{m\times m}$ can be decomposed as an upper trangular matrix.
\begin{theorem} [Schur triangularization]
    Given $A\in\mathbb{C}^{m\times m}$ with eigenvalues $\lambda_1,\lambda_2,...,\lambda_m\in\mathbb{C}$ there 
    exists a unitary matrix $U\in\mathbb{C}^{m\times m}$ such that 
    \[
        U^HAU = T \Longleftrightarrow A=UTU^H
    \]
    where $T$ is an upper triangular matrix with $[T]_{ii}=\lambda_i$, $i=1,2,...,m$
\end{theorem}
If $A\in\mathbb{C}^{m\times m}$, by Schur triangularization, there exists unitary $U\in\mathbb{C}^{m\times m}$ s.t. 
\[
    U^HAU = T 
\]
Then by taking $H$, we have
\[
    \underset{(=U^HAU)}{U^HA^HU}=T^H
\]
$\Longrightarrow$
\[
    T=T^H
\]
Since $T$ is an upper triangular matrix, so $T^H$ is a lower triangular matrix. Therefore, $T$ is diagonal.
$\Longrightarrow$
\[
    U^HAU = T
\]
Thus, $A$ is diagonalizable.

\begin{proof} for Schur Triangularization (\textbf{Theorem 3}) \\
    \emph{This proof is also an algorithm.} \\
    Let $v_1\in\mathbb{R}^m$ be the eigenvalue of $A$ associated with $\lambda_1$, i.e., $Av_1=\lambda_1v_1$. 
    Without losing generality, we can assume $v_1^Hv_1=1$ and $\lambda_1\geq \lambda_2\geq ... \geq \lambda_m$

    Construct a unitary matrix $U_1$
    \[
        U_1 = \begin{bmatrix}v_1 & V_1\end{bmatrix} \in\mathbb{C}^{m\times m} 
    \]
    where $V_1\in\mathbb{C}^{m\times (m-1)}$. $U_1$ being unitary indicates \[v_1^HV_1=\overrightarrow{0}^T\]
    Then we have 
    \[
        U_1^HAU_1 = \begin{bmatrix} v_1^H \\ V_1^H \end{bmatrix} A \begin{bmatrix} v_1 & V_1 \end{bmatrix} 
        = \begin{bmatrix}
            v_1^HAv_1 & v_1^HAV_1 \\
            V_1^HAv_1 & V_1^HAV_1
        \end{bmatrix}
    \]
    where 
    \[
        \left\{\begin{array}{l}
            v_1^HAv_1=\lambda_1 \\
            v_1^HAV_1 \neq \overrightarrow{0} \\
            V_1^HAv_1 = \overrightarrow{0}
        \end{array}\right.
    \]
    So,
    \[
        U_1^HAU_1 = \begin{bmatrix}
            \lambda_1 & \times \\
            0 & V_1^HAV_1
        \end{bmatrix}
    \]
    Define
    \[
        A_1 \triangleq V_1^HAV_1 \in \mathbb{C}^{(m-1)\times (m-1)}
    \]
    Then
    \[
        \begin{array}{ll}
            P_{A}(\lambda) & = det(A-\lambda I) \\
            & = det(U_1^HAU_1-\lambda I)^\dagger  \\ 
            & = det\left(\begin{bmatrix}
                \lambda_1 & \times \\
                0 & A_1
            \end{bmatrix}-\lambda I\right) \\
            & = det\left(\begin{bmatrix}
                \lambda_1-\lambda & \times \\
                0 & A_1-\lambda I
            \end{bmatrix}\right) \\
            & = (\lambda_1-\lambda)\underbrace{det(A_1-\lambda I)}_{=P_{A_1}(\lambda)}
        \end{array}
    \]
    $^\dagger$: $A=U_1(U_1^HAU_1)U_1^{-1}$ so $A$ is similar to $U_1^HAU_1$ and hence $A$ and $U_1^HAU_1$
    have same eigenvalues.

    Therefore, $A_1$ has eigenvalues $\lambda_2,\lambda_3,...,\lambda_m$ \\
    Let $v_2\in\mathbb{C}^{m-1}$ be the eigenvector of $A_1$ associated with $\lambda_2$. \\
    Construct a unitary matrix $U_2$
    \[
        U_2 = \begin{bmatrix}v_2 & V_2\end{bmatrix} \in\mathbb{C}^{(m-1)\times (m-1)} 
    \]
    where $V_2\in\mathbb{C}^{(m-1)\times (m-2)}$. $U_2$ being unitary indicates \[v_2^HV_2=\overrightarrow{0}^T\]\\ 
    $\Longrightarrow$ 
    \[
        U_2^HA_1U_2 = \begin{bmatrix}
            \lambda_2 & \times  \\
            0 & V_2^HA_1V_2
        \end{bmatrix}
    \]
    Let $\tilde{U}_2=\begin{bmatrix}
        1 & 0^T \\ 0 & U_2
    \end{bmatrix}$, then $\tilde{U}_2$ is unitary. \\
    So,
    \[
        \begin{array}{ll}
            \tilde{U}_2^H[U_1^HAU_1]\tilde{U}_2 & = \tilde{U}_2^H \begin{bmatrix}
                \lambda_1 & \times \\
                0 & A_1
            \end{bmatrix} \tilde{U}_2 \\
        & = \begin{bmatrix}
            \lambda_1 & \times \\
            0 & U_2^HA_1U_2
        \end{bmatrix} \\
        & = \begin{bmatrix}
            \lambda_1 & \times \\
            0 & \begin{bmatrix}
                \lambda_2 & \times  \\
                0 & V_2^HA_1V_2
                \end{bmatrix}
            \end{bmatrix} \\
        & = \begin{bmatrix}
            \lambda_1 & \times    & \times \\
            0         & \lambda_2 & \times \\
            0         & 0         & V_2^HA_1V_2
        \end{bmatrix}
        \end{array}
    \]
    Repeat this operations and we can finally get 
    \[
        \begin{array}{l}
        \tilde{U}_m^H\cdots\tilde{U}_3^H\tilde{U}_2^H(U_1A \underbrace{U_1)\tilde{U}_2\tilde{U}_3\cdots\tilde{U}_m}_{\triangleq U \text{ \rm unitary}} \\
        = \begin{bmatrix}
            \lambda_1   & \times    & \times    & \times    \\
                        & \lambda_2 & \times    & \times    \\
                        &           & \ddots    & \times    \\
                        &           &           & \lambda_m
        \end{bmatrix}\triangleq T
        \end{array}
    \]
    Therefore,
    \[
        U^HAU = T
    \]
\end{proof}
\subsection{Variational Characterization of Eigenvalues}
\subsubsection{Rayleigh quotient}
\begin{definition}[Rayleigh quotient]
    \[
        R(A,x) = \frac{x^HAx}{\|x\|_2^2}=\left(\frac{x}{\|x\|_2}\right)^HA\left(\frac{x}{\|x\|_2}\right)
    \]
\end{definition}
\begin{theorem}[Rayleigh-Ritz theorem]
    Let $A\in\mathbb{C}^{m\times m}$ be Hermitian, then
    \[
        \begin{array}{ll}
            \lambda_{\max}(A)    & = \max_{x\in\mathbb{C}^m} \frac{x^HAx}{\|x\|_2^2} \\
                                 & = \max_{x\in\mathbb{C}^m} x^HAx \quad (\|x\|_2=1) \\
            \lambda_{\min}(A)    & = \min_{x\in\mathbb{C}^m} \frac{x^HAx}{\|x\|_2^2}
        \end{array}
    \]
    and 
    \[
        \lambda_{\min}(A)\cdot\|x\|_2^2 \leq x^HAx \leq \lambda_{\max}(A)\cdot\|x\|_2^2
    \] 
\end{theorem}
\begin{proof} \\
    According to the EVD of $A$,
    \[
        A = V \Lambda V^H = \sum_{i=1}^{m} (\lambda_i \cdot v_i v_i^H)
    \]
    $\Longrightarrow$
    \[
        \begin{array}{ll}
            x^HAx & = x^H V \Lambda V^H x  \\
            & = \sum_{i=1}^{m} (\lambda_i \cdot x^H v_i v_i^H x)  \\
            & = \sum_{i=1}^{m} (\lambda_i \cdot |v_i^Hx|^2)  \\ 
            & \leq \lambda_{\max}(A) \cdot \sum_{i=1}^{m} |v_i^Hx|^2
        \end{array}
    \]
    Since
    \[
        \sum_{i=1}^{m} |v_i^Hx|^2 = x^HVV^Hx = \|x\|_2^2
    \]
    So,
    \[
        x^HAx \leq \lambda_{\max}(A) \cdot \|x\|_2^2
    \]
    $\Longrightarrow$ 
    \[
        \frac{x^HAx}{\|x\|_2^2} \leq \lambda_(\max)(A)
    \]
    The equality holds only when $x=v_1$, which is an eigenvector associated with $\lambda_{\max}(A)$ and is called the principal eigenvector.
\end{proof}

\subsubsection{Courant-Fischer theorem}
\begin{theorem}[Courant-Fischer theorem]
    Let $A\in\mathbb{C}^{m\times m}$ be Hermitian and $\lambda_1\geq \lambda_2 \geq \cdots \geq \lambda_m$ are ordered eigenvalues. Then
    \[
        \begin{array}{llc}
            \lambda_k &=&  \underset{\omega_1,\omega_2,...,\omega_{k-1}}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,\omega_2,...,\omega_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2}\right\}\\
            &=&    \underset{\omega_{k+1},\omega_{k+2},...,\omega_{m}}{\max} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_{k+1},\omega_{k+2},...,\omega_{m}}{\min} \frac{x^HAx}{\|x\|_2^2}\right\}        
        \end{array}
    \]
\end{theorem}
\textbf{Proof ideas}: \\
1.
\[
    \underset{x\in\mathbb{C}^m,x\bot v_1,v_2,...,v_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2} = \lambda_k
\]
where $v_k$ is the eigenvector associated with $\lambda_k$ of $A$ \\
2.
\[
    \underset{x\in\mathbb{C}^m,x\bot \omega_1,\omega_2,...,\omega_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2} \geq \lambda_k
\]
1+2 $ \Longrightarrow$
\[
    \underset{\omega_1,\omega_2,...,\omega_{k-1}}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,\omega_2,...,\omega_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2}\right\}
    =\lambda_k
\]
attained when $\omega_i = v_i$ ($i=1,...,k-1$)


\begin{proof} \\
    1. \\
    Since $A=V\Lambda V^H=\sum_{i=1}^m\lambda_iv_iv_i^H$,
    \[
        \begin{array}{l}
            \underset{x\in\mathbb{C}^m,x\bot v_1,v_2,...,v_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2} \\
            = \underset{x\in\mathbb{C}^m,x\bot v_1,v_2,...,v_{k-1}}{\max} \frac{\sum_{i=1}^m\lambda_i|v_i^Hx|^2}{\|x\|_2^2} \\
            = \underset{x\in\mathbb{C}^m,x\bot v_1,v_2,...,v_{k-1}}{\max} \frac{\sum_{i=k}^m\lambda_i|v_i^Hx|^2}{\|x\|_2^2} \\
            \leq \underset{x\in\mathbb{C}^m}{\max} \frac{\sum_{i=k}^m\lambda_i|v_i^Hx|^2}{\|x\|_2^2} \quad(\text{\rm Relax constraint}) \\
            \leq \underset{x\in\mathbb{C}^m}{\max} \frac{\lambda_k\sum_{i=k}^m|v_i^Hx|^2}{\|x\|_2^2} ^\dagger \\
            \leq \lambda_k \quad (\text{\rm equality attained if } x=v_k)
        \end{array}
    \]
    $^\dagger$: $\sum_{i=k}^m|v_i^Hx|^2=x^HVV^Hx=\|x\|_2^2$.

    Therefore, 
    \[
        \underset{x\in\mathbb{C}^m,x\bot v_1,v_2,...,v_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2} = \lambda_k
    \]
    2. \\
    Let $y=V^Hx$
    \[
        \begin{array}{l}
            \underset{x\bot \omega_1,...,\omega_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2} \\
            = \underset{x\bot \omega_1,...,\omega_{k-1}}{\max} \frac{x^HV\Lambda V^H x}{\|x\|_2^2} \\
            = \underset{y\bot V^H\omega_1,...,V^H\omega_{k-1}}{\max} \frac{y^H\Lambda y}{\|y\|_2^2} \\
            \geq \underset{y\bot V^H\omega_1,...,V^H\omega_{k-1},y_{k+1}=y_{k+2}=\cdots=y_{m}=0,\|y\|_2=1}{\max} y^H\Lambda y \\
            = \underset{y\bot V^H\omega_1,...,V^H\omega_{k-1},|y_1|^2+|y_2|^2+\cdots+|y_k|^2=1}{\max} \sum_{i=1}^k \lambda_i|y_i|^2 \\
            \geq \lambda_k 
        \end{array}
    \]

\end{proof}

\subsubsection{Weyl's inequality}
For $A, B\in\mathbb{C}^{m\times m}$ be two Hermitian matrices. \\ Then
\begin{enumerate}
    \item $\lambda_k(A) + \lambda_m(B) \leq \lambda_k(A+B) \leq \lambda_k(A) + \lambda_1(B)$
    \item $\lambda_{k+1}(A) \leq \lambda_k(X\pm zz^H) \leq \lambda_{k-1}(A)$
    \item $\lambda_{k+r}(A) \leq \lambda_k(A+B) \leq \lambda_{k-r}(A)$ where $rank(B)\leq r$
    \item $\lambda_{j+k-1}(A+B) \leq \lambda_j(A) + \lambda_k(B) \leq \lambda_{j+k-m}(A+B)$
    \item Let $A_I$ be the principal submatrix of $A$ associated with subset $I=\{i_1,i_2,...,i_r\}\subseteq \{1,2,...,m\}$
    \[
        \lambda_{k+m-r}(A) \leq \lambda_k(A_I) \leq \lambda_k(A)
    \]
    \item Let $U\in\mathbb{C}^{m\times r]}$ be semiunitary
    \[
        \lambda_{k+m-r} \leq \lambda_k(U^HAU) \leq \lambda_k(A)
    \]
\end{enumerate}
\begin{proof} 1. 
    \[
        \lambda_k(A+B) = \underset{\omega_1,...,\omega_{k-1}}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,...,\omega_{k-1}}{\max} \frac{x^H(A+B)x}{\|x\|_2^2}\right\}
    \]
    where
    \[
        \frac{x^H(A+B)x}{\|x\|_2^2} = \frac{x^HAx}{\|x\|_2^2} + \underbrace{\frac{x^HBx}{\|x\|_2^2}}_{\leq \lambda_1(B)}
    \]
    So,
    \[
        \begin{array}{l}
            \lambda_k(A+B) \\
            \leq \underbrace{\underset{\omega_1,...,\omega_{k-1}}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,...,\omega_{k-1}}{\max} \frac{x^HAx}{\|x\|_2^2}\right\}}_{=\lambda_k(A)} + \lambda_1(B)
        \end{array}
    \]
    Therefore,
    \[
        \lambda_k(A+B) \leq \lambda_k(A) + \lambda_1(B)
    \]
\end{proof}

\begin{proof} 2.
    \[
        \begin{array}{l}
            \lambda_k(A \pm zz^H) \\ 
            = \underset{\omega_1,...,\omega_{k-1}}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,...,\omega_{k-1}}{\max} \frac{x^H(A\pm zz^H)x}{\|x\|_2^2}\right\} \\
            \geq \underset{\omega_1,...,\omega_{k-1}}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,...,\omega_{k-1},\boxed{z}}{\max} \frac{x^H(A\pm zz^H)x}{\|x\|_2^2}\right\} \\
            = \underset{\omega_1,...,\omega_{k-1},\omega_k=z}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,...,\omega_{k-1},\omega_k}{\max} \frac{x^HAx}{\|x\|_2^2}\right\} \\
            \geq \underset{\omega_1,...,\omega_{k-1},\omega_k}{\min} \left\{\underset{x\in\mathbb{C}^m,x\bot \omega_1,...,\omega_{k-1},\omega_k}{\max} \frac{x^HAx}{\|x\|_2^2}\right\} \\
            = \lambda_{k+1}(A)
        \end{array}
    \]
\end{proof}

\begin{proof} 3.
    \[
        \because B=\sum_{i=1}^r \lambda_i(B)u_iu_i^H
    \]
    \[
        \therefore A+B = A + \sum_{i=1}^{r-1} \lambda_i(B)u_iu_i^H + \lambda_r(B)u_ru_r^H
    \]
    Define $B^{r-k}\triangleq \sum_{i=1}^{r-k} \lambda_i(B)u_iu_i^H$ $k=0,1,...,r$
    \[
        \lambda_{k+1}(A+B^{r-1}) \leq \lambda_k(A + B^{r-1} + \lambda_r(B)u_ru_r^H) \leq \lambda_{k-1}(A+B^{r-1})
    \]
    Do again and we get 
    \[
        \begin{array}{ll}
            \lambda_{k+2}(A+B^{r-2}) & \leq \lambda_{k+1}(A + B^{r-2} + \lambda_{r-1}(B)u_{r-1}u_{r-1}^H) \\ &= \lambda_{k+1}(A+B^{r-1})
        \end{array}
    \]
    Repeat $r$ times, we can get 
    \[
        \lambda_{k+r}(A) \leq \lambda_k(A+B) \leq \lambda_{k-r}(A)
    \]
    where $rank(B)\leq r$
\end{proof}

\begin{proof} 4. \\
    Define $A_{j-1}=\sum_{i=1}^{j-1}\lambda_i(A)v_iv_i^H$ (if $A=V\Lambda V^H $). \\
    Define $B_{k-1}=\sum_{i=1}^{k-1}\lambda_i(B)u_iu_i^H$ (if $B=U\Lambda U^H $). \\
    Then 
    \[
        \lambda_j(A) = \lambda_1(A-A_{j-1})
    \]
    \[
        \lambda_k(B) = \lambda_1(B-B_{k-1})
    \]
    $\Longrightarrow$
    \[
        \begin{array}{ll}
            \lambda_j(A)+\lambda_k(B)   & = \lambda_1(A-A_{j-1})+\lambda_1(B-B_{k-1}) \\
                                        & \geq \lambda_1(A+B-(A_{j-1}+B_{k-1})) \\
                                        & \geq \lambda_{j+k-1}(A+B)^\dagger
        \end{array}
    \]
    $^\dagger$: $rank((A_{j-1}+B_{k-1})\leq j+k-2$ and from 3.
\end{proof}

\begin{proof} 5. 
    \[
        \begin{array}{l}
            \lambda_k(A) \\
            = \underset{\omega_1,...,\omega_{k-1}}{\min} \left\{\underset{x\bot \omega_1,...,\omega_{k-1},\|x\|_2=1}{\max} x^HAx \right\} \\
            \geq \underset{\omega_1,...,\omega_{k-1}}{\min} \left\{\underset{x\bot \omega_1,...,\omega_{k-1},\|x\|_2=1,x\in span\{e_{i_1},...,e_{i_r}\}}{\max} x^HAx \right\} ^\dagger \\
            = \underset{v_1,...,v_{k-1}}{\min} \left\{\underset{y\bot v_1,...,v_{k-1},\|y\|_2=1\}}{\max} y^HAy \right\} ^\ddagger \\
            = \lambda_k(A_I)
        \end{array}
    \]
    $^\dagger$: $e_i=\begin{bmatrix}
        \textbf{0}  \\ 1  \\ \textbf{0}
    \end{bmatrix}$, only the $i$th entry equal to 1. \\
    $^\ddagger$: Define $y\in \mathbb{C}^{r}$, $y_\ell = x_{i_\ell}$ ($\ell = 1,...,r$) and take $v_i\in\mathbb{C}^r$, $(v_i)_\ell = (\omega_i)_{i_\ell}$
\end{proof}

\begin{proof} 6. \\
    Let $\tilde{U}=[\underbrace{U}_{\in\mathbb{C}^{m\times r}}, \underbrace{\bar{U}}_{\in\mathbb{C}^{m\times r}}]$ be unitary. Then
    \[
        \tilde{U}^HA\tilde{U} = \begin{bmatrix}
            U^HAU  & \times \\
            \times & \times
        \end{bmatrix}
    \]
    where $U^HAU$ is the principal submatrix of $\tilde{U}^HA\tilde{U}$ with $I={1,2,...,r}$\\
    Therefore, 
    \[
        \lambda_k(A) = \lambda_k(\tilde{U}^HA\tilde{U}) \geq \lambda_k(U^HAU)
    \]
\end{proof}

\subsection{The Power Method}
The power method is a simple method to obtain eigenvector/eigenvalue of a matrix, particularly for large scale problem.
\begin{algorithm}[Power Method]
Given a matrix $A\in\mathbb{C}^{m\times m}$ and $v^0\in\mathbb{C}^m$
\[
    v^{(k+1)}\leftarrow \frac{A\cdot v^{(k)}}{\|A\cdot v^{(k)}\|_2}
\]
until a stopping condition is met.
\end{algorithm}
$\Longrightarrow$ $v^{(k+1)}$ takes as an estimate of the \textbf{principle eigenvector} of $A$. \\
Assumption: 
\begin{itemize}
    \item [-] $A$ is diagonalizable (in particular, $A$ is Hermitian (symmetric)). $A=V\Lambda V^H$.
    \item [-] $|\lambda_1|\geq |\lambda_2|\geq ... \geq |\lambda_m|$ and $|\lambda_1|>|\lambda_2|$
\end{itemize}
$v^{(k+1)} = \frac{A\cdot v^{(k)}}{\|A\cdot v^{(k)}\|_2} = \frac{A^{k+1}\cdot v^{(0)}}{\|A^{k+1}\cdot v^{(0)}\|_2}$




Consider $A^kx$, $x$ satisfies $[V^Hx]_1\neq 0$. ($x$ is $v^{(0)}$)\\
Let $\alpha = V^Hx$ \\
\[
    A^k\cdot x = (V\Lambda^kV^H)\cdot x = V\Lambda^k \alpha 
\]
Let $V=[v_1,v_2,...,v_m]$
\[
    \begin{array}{ll}
        A^k\cdot x  &= V\Lambda^k \alpha \\
                    &= \sum_{i=1}^mv_i\cdot \lambda_i^k \cdot \alpha_i \\
                    &= v_1(\lambda_1\alpha_1) + \sum_{i=2}^m\alpha_i\lambda_i^kv_i \\
                    &= \lambda_1^k\alpha_1 (v_1+\underbrace{\sum_{i=2}^m \frac{\alpha_i}{\alpha_1} \left(\frac{\lambda_i}{\lambda_1}\right) ^kv_i}_{\triangleq r_k(\textbf{\rm residule})})
    \end{array}
\]
$\Longrightarrow$
\[
    \begin{array}{ll}
        \|r_k\|_2   &=      \|\sum_{i=2}^m \frac{\alpha_i}{\alpha_1} \left(\frac{\lambda_i}{\lambda_1}\right) ^kv_i\|_2 \\
                    &\leq   \sum_{i=1}^m \frac{\alpha_i}{\alpha_1} \cdot \left(\frac{\lambda_i}{\lambda_1}\right) ^k \cdot \underbrace{\|v_i\|_2}_{=1} \\
                    &\leq   \underbrace{|\frac{\lambda_2}{\lambda_1}|^k}_{<1}\cdot \sum_{i=1}^m |\frac{\alpha_i}{\alpha_1}| \to 0 \text{ \rm as } k\to \infty

    \end{array}
\]
Consider $c_k=\frac{|\alpha_1||\lambda_1^k|}{\alpha_1\lambda_1^k}$ and $\lim_{k\to\infty} c_k\cdot \frac{A^k\cdot x}{\|A^k\cdot x\|_2}$
\[
    \begin{array}{l}
        c_k\cdot \frac{A^k\cdot x}{\|A^k\cdot x\|_2}    \\
        = \frac{|\alpha_1||\lambda_1^k|}{\alpha_1\lambda_1^k}\cdot \frac{1}{\|A^k\cdot x\|_2^\dagger}\cdot \lambda_1^k\alpha_1\left(v_1+\sum_{i=2}^m\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right) \\
        = \frac{|\alpha_1||\lambda_1^k|}{\sqrt{\sum_{i=1}^m (\lambda_i^k)^2 \alpha_i^2}}^\ddagger \cdot \left(v_1+\underbrace{\sum_{i=2}^m\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^kv_i}_{\to 0 \text{ \rm as }k\to \infty}\right) 
    \end{array}
\]
$^\dagger$: $\|A^kx\|_2 = \|V\Lambda^k V^H x\|_2 = \|V\Lambda^k \alpha\|_2 = \|\Lambda^k \alpha\|_2 = \sqrt{\sum_{i=1}^m(\lambda_i^k \alpha_i)^2}$ \\
$^\ddagger$:
$
    \begin{array}{ll}
        \frac{|\alpha_1||\lambda_1^k|}{\sqrt{\sum_{i=1}^m (\lambda_i^k)^2 \alpha_i^2}}  &= \frac{|\alpha_1||\lambda_1^k|}{\sqrt{(\lambda_1^k)^2 \alpha_1^2 + \sum_{i=2}^m (\lambda_i^k)^2 \alpha_i^2}} \\
                                                                                        &= \frac{1}{\sqrt{1+\sum_{i=2}^m \left(\frac{\lambda_i}{\lambda_1}\right)^{2k} \left(\frac{\alpha_i}{\alpha_1}\right)^2}}
    \end{array}
$\\
Since $\sum_{i=2}^m \left(\frac{\lambda_i}{\lambda_1}\right)^{2k} \left(\frac{\alpha_i}{\alpha_1}\right)^2 \to 0$ as $k\to \infty$, so $\frac{|\alpha_1||\lambda_1^k|}{\sqrt{\sum_{i=1}^m (\lambda_i^k)^2 \alpha_i^2}}\to 1$ as $k\to \infty$.

Therefore, $\lim_{k\to\infty} c_k\cdot \frac{A^k\cdot x}{\|A^k\cdot x\|_2}\to v_1$.

\textbf{Deflation}:
\[
    \begin{array}{ll}
        A &= \sum_{i=1}^{m} \lambda_i v_i v_i^H \\
          &= \lambda_1 v_1 v_1^H + \sum_{i=2}^{m} \lambda_i v_i v_i^H
    \end{array}
\]
Obtain $A^{(1)}=A-\lambda_1v_1v_1^H$ and apply power method to obtain $v_2$. \\
Obtain $A^{(2)}=A-\lambda_2v_2v_2^H$ and apply power method to obtain $v_3$. \\
...
\newpage
\end{multicols}